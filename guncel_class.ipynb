{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veri Seti Hakkında Bilgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: researchpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from researchpy) (1.15.4)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from researchpy) (0.9.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from researchpy) (1.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from researchpy) (0.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->researchpy) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->researchpy) (2018.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas->researchpy) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "!pip install researchpy\n",
    "import researchpy as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Information():\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    \n",
    "    def info(self):\n",
    "        print(data.info())\n",
    "        \n",
    "    \n",
    "    def copy(self):\n",
    "        data1 = data.copy()\n",
    "        data2 = data.copy()\n",
    "    \n",
    "    def describe(self):\n",
    "        print(data.describe())\n",
    "        \n",
    "        print(\"**********************************\")\n",
    "        print(\" Loan Değişkeninin incelenmesi\")\n",
    "        print(\"Ortalama: \" + str(data[\"loan\"].mean()))\n",
    "        print(\"Dolu Gözlem Sayısı: \" + str(data[\"loan\"].count())) \n",
    "        print(\"Maksimum Değer: \" + str(data[\"loan\"].max()))\n",
    "        print(\"Minimum Değer: \" + str(data[\"loan\"].min()))\n",
    "        print(\"Medyan: \" + str(data[\"loan\"].median()))\n",
    "        print(\"Standart Sapma: \" + str(data[\"loan\"].std()))\n",
    "        \n",
    "    def shape(self):\n",
    "        \n",
    "        #type(data)\n",
    "        print(data.axes)\n",
    "        #print(self.data.axes)\n",
    "        #print(\" ndim = \" + str(data.ndim))\n",
    "        print(\" Veri şekli = \" + str(data.shape))\n",
    "        print(\" Veri büyüklüğü = \" + str(data.size))\n",
    "        #type(data.values)\n",
    "    \n",
    "    def head(self,row = None):\n",
    "        \n",
    "        print(data.head(5))\n",
    "        # print(data.tail(3))\n",
    "        \n",
    "    def hedef_degisken(self):\n",
    "        \n",
    "        countNo = len(data[data.bad == 0])\n",
    "        countYes = len(data[data.bad == 1])\n",
    "        print(\"Risk barındırmayan müşterilerin yüzdesi : {:.2f}%\".format((countNo / (len(data.bad))*100)))\n",
    "        print(\"Risk barındıran müşterilerin yüzdesi : {:.2f}%\".format((countYes / (len(data.bad))*100)))\n",
    "        \n",
    "        print(\"************************************\")\n",
    "        \n",
    "        print(data.groupby('bad').mean().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Görselleştirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization():\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    def visual(self):\n",
    "        data[\"job\"].value_counts().plot.barh().set_title(\"Job Değişkeninin Sınıf Frekansları\");\n",
    "    \n",
    "    def countplot(self):\n",
    "        sns.countplot(x=\"bad\", data=data, palette=\"bwr\")\n",
    "        plt.show()\n",
    "        \n",
    "    def figure(self):\n",
    "        fig, ax = plt.subplots(1, 2, figsize = (18, 8))\n",
    "        sns.barplot(x = \"job\", y = \"loan\", hue = \"bad\", data = data, ax = ax[0])\n",
    "        ax[0].set_ylabel(\"loan\")\n",
    "        sns.catplot(x = \"job\", y = \"loan\", data = data, ax = ax[1]) \n",
    "        ax[1].set_ylabel(\"Loan\")\n",
    "        \n",
    "    def distplot(self):\n",
    "        sns.distplot(data.loan, kde = False);\n",
    "        \n",
    "    def pairplot(self):\n",
    "        \n",
    "        sns.pairplot(data);\n",
    "        \n",
    "        \n",
    "    def pivotTable(self):\n",
    "        \n",
    "        print(data.pivot_table(\"loan\",index = \"bad\",columns = \"reason\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# İSTATİSTİK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stats():\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    def imports(self):\n",
    "        \n",
    "        import scipy.stats as stats\n",
    "        import statsmodels.stats.api as sms\n",
    "        import pylab\n",
    "        from scipy.stats.stats import pearsonr\n",
    "        import researchpy as rp\n",
    "        \n",
    "    \n",
    "    def table(self):\n",
    "        \n",
    "        print(rp.summary_cont(data[[\"loan\",\"value\"]])) \n",
    "        \n",
    "        print(\"*******************************\")\n",
    "        \n",
    "        print(rp.summary_cat(data[[\"bad\",\"job\",\"reason\"]]))\n",
    "        \n",
    "        print(\"*******************************\")\n",
    "        \n",
    "        print(data[[\"loan\",\"value\"]].corr()) # Korelasyon\n",
    "        \n",
    "    def shapiro(self):\n",
    "        \n",
    "        import scipy.stats as stats\n",
    "        from scipy.stats import shapiro\n",
    "        test, p_value = shapiro(data[\"loan\"])\n",
    "        \n",
    "        print(\"Varsayım kontrolünde pvalue = \" + str(p_value))\n",
    "        \n",
    "        \n",
    "        if p_value > 0.05:\n",
    "            print(\"Örnek dağılımı ile teorik normal dağılım arasında ist. ol. anl. bir fark. yoktur.\")\n",
    "        else:\n",
    "            print(\"Örnek dağılımı ile teorik normal dağılım arasında ist. ol. anl. bir fark. vardır.\")\n",
    "        \n",
    "        test_istatistigi, pvalue = stats.ttest_1samp(data[\"loan\"], popmean = 10000)\n",
    "        print(\"pvalue = \" + str(pvalue))\n",
    "        \n",
    "        if pvalue > 0.05 :\n",
    "            print(\"H0 hipotezimiz reddedilemez.\")\n",
    "        else:\n",
    "            print(\"H0 hipotezimiz reddelir.\")   \n",
    "        \n",
    "        \n",
    "        \n",
    "    def qqplot(self):\n",
    "        import pylab\n",
    "        import scipy.stats as stats\n",
    "        stats.probplot(data[\"loan\"], dist=\"norm\", plot=pylab)\n",
    "        pylab.show()\n",
    "        \n",
    "    def korelasyon_analiz(self):\n",
    "        \n",
    "        # müşteri hakkındaki kötü raporlar arttığında ne olduğunu gözlemlemek istiyorum.\n",
    "        data.plot.scatter(\"loan\",\"value\");\n",
    "\n",
    "        print(data[\"loan\"].corr(data[\"value\"])) # parametrik karşılığı\n",
    "        \n",
    "        # Hedef değişken BAD için analiz\n",
    "        \n",
    "        print(data.corr()['bad'].abs().sort_values(ascending=False))\n",
    "        \n",
    "        \n",
    "    def korelasyon_map(self):\n",
    "        \n",
    "        corr = data.corr()\n",
    "        # print(corr)\n",
    "        # sns.heatmap(corr, \n",
    "        #xticklabels=corr.columns.values,\n",
    "        #yticklabels=corr.columns.values)\n",
    "        \n",
    "        fig,ax = plt.subplots(figsize=(15, 15))\n",
    "        sns.heatmap(data.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ön İşleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    \n",
    "    def outlier(self):\n",
    "        \n",
    "        t_data = data.dropna() # Eksik gözlemleri sildik. Aykırı gözlemleri rahatça görebilmek için.\n",
    "        print(t_data.head())\n",
    "        \n",
    "        # Bir tane gözlemimi alıyorum.\n",
    "        data_loan = data[\"loan\"]\n",
    "        sns.boxplot(x = data_loan);\n",
    "        \n",
    "        Q1 = data_loan.quantile(0.25)\n",
    "        Q3 = data_loan.quantile(0.75)\n",
    "        IQR = Q3-Q1\n",
    "       \n",
    "        \n",
    "        print(\" IQR = \" + str(Q3-Q1))\n",
    "        print(\" Alt Sınır = \" + str(Q1- 1.5*IQR))\n",
    "        print(\" Üst Sınır = \" + str(Q3 + 1.5*IQR))\n",
    "        print(data_loan.median())\n",
    "       \n",
    "        # Daha sonra aykırı değeri Baskılama ve ya Ortalama yöntemi ile doldurma işlemleri\n",
    "    \n",
    "    def MissingValue(self):\n",
    "        \n",
    "        print(data.isnull().sum())\n",
    "        #print(data.isnull())\n",
    "        #print(data[data.isnull().any(axis = 1)]) # Eksik değer olan satırlara erişmek\n",
    "        \n",
    "        # Bir gözlemi örnek alıyorum.\n",
    "        # data[\"loan\"]\n",
    "        # data[\"loan\"].mean()\n",
    "        # print(data[\"loan\"].fillna(data[\"loan\"].mean())) # fillna doldurma işlemi için yapılıyor.\n",
    "        \n",
    "        # data.isna().values.any() #Kalan NaN değeri var mı kontrol etmek için\n",
    "     \n",
    "    def dropNa(self):\n",
    "        data.dropna(inplace = True) # tüm eksik gözlemlerin silinmesi\n",
    "        \n",
    "     \n",
    "    \n",
    "    def fillna(self):\n",
    "        data.fillna(data.mean()[:],inplace = True)\n",
    "        \n",
    "        # kategorik\n",
    "        \n",
    "        print(\" En sık tekrarlanan sebep:\" + str(data1[\"reason\"].mode()[0]))\n",
    "        print(\" En sık tekrarlanan iş:\" + str(data1[\"job\"].mode()[0]))\n",
    "        data1[\"job\"].fillna(data1[\"job\"].mode()[0],inplace = True)\n",
    "        data1[\"reason\"].fillna(data1[\"reason\"].mode()[0], inplace = True)\n",
    "        \n",
    "        \n",
    "    def scale(self):\n",
    "        \n",
    "        # gerekmesi durumunda içerisinde çok büyük değerler olan gözlemler için yapılabilir.\n",
    "        pass\n",
    "    \n",
    "    def normalize(self):\n",
    "        \n",
    "        from sklearn import preprocessing\n",
    "        preprocessing.normalize(X)\n",
    "        \n",
    "        \n",
    "    def dummies(self):\n",
    "        \n",
    "        cat = pd.get_dummies(data2[['reason', 'job']])\n",
    "        X = pd.concat([X, cat[[\"reason_DebtCon\",\"reason_HomeImp\",\"job_Mgr\" ,\"job_Office\",\"job_Other\",\"job_ProfEx\",\"job_Sales\",\"job_Self\"]]], axis = 1)\n",
    "        print(X.head)\n",
    "        \n",
    "        \n",
    "    def onhazirlik(self): # burada modele sokulacak target değişken ve çıkarılması gereken değişkenler \n",
    "        \n",
    "        y = data[\"bad\"]\n",
    "        X = data.drop([\"bad\",\"reason\",\"job\"],axis = 1)\n",
    "        \n",
    "        print(X.head(3))\n",
    "        \n",
    "        return y,X\n",
    "    def train_test_split(self):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state = 42)\n",
    "        \n",
    "        \n",
    "    def drop(self):\n",
    "        \n",
    "        # Anlamsız değişkenler ve ya herhangi bir değişkenin çıkarılması işlemi\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model İşlemleri ve Model Performansını Değerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problemime göre kullanacağım algoritmaları bu fonksiyon altında çağıracağım.\n",
    "# Regression, Classification, Clustering\n",
    "\n",
    "class ModelSelect():\n",
    "    def __init__(self,data,X,y):\n",
    "        self.data = data\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def Log(self):\n",
    "        loj = sm.Logit(y, X)\n",
    "        loj_model = loj.fit()\n",
    "        loj_model.summary()\n",
    "        \n",
    "    def Sklearn_Log(self):\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        loj = LogisticRegression(solver = \"liblinear\") # solver = hangi fonksiyon ile işlem yapacaksın? farklı solver'lar ile deneyip bakmak da fayda var.\n",
    "        loj_model = loj.fit(X,y)\n",
    "        \n",
    "        \n",
    "    def Log_predict(self): # test ve train diye ayırmadan\n",
    "        \n",
    "        loj_model.predict_proba(X)[0:10][:,0:2] # OLASILIKLAR\n",
    "        y_probs = loj_model.predict_proba(X)\n",
    "        y_probs = y_probs[:,1]\n",
    "        y_probs[0:10]\n",
    "        y_pred = [1 if i > 0.3 else 0 for i in y_probs] \n",
    "        \n",
    "    \n",
    "    def Sklearn_Log_split(self): # train ve test\n",
    "        \n",
    "        loj = LogisticRegression(solver = \"liblinear\")\n",
    "        loj_model = loj.fit(X_train,y_train)\n",
    "        # y_pred = loj_model.predict(X_test)\n",
    "        y_probs = loj_model.predict_proba(X_test)\n",
    "        y_probs = y_probs[:,1]\n",
    "        y_probs[0:10]\n",
    "        y_pred = [1 if i > 0.3 else 0 for i in y_probs]\n",
    "             \n",
    "  \n",
    "    \n",
    "    def ModelTuning(self):\n",
    "        \n",
    "        # GridSearch\n",
    "        # best parameters\n",
    "        \n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrik Değerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performansını değerlendireceğim class\n",
    "# MSE,RMSE, f1 score, AUC-CORVE, correlation matrix\n",
    "\n",
    "\n",
    "class Evaluate():\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    def acc(self): # split etmeden doğruluk değeri\n",
    "        \n",
    "        print( \"acc :\" + str(accuracy_score(y, y_pred)))\n",
    "    \n",
    "    def acc_split(self): # train-test split\n",
    "        \n",
    "        print(\" train acc : \" + str(accuracy_score(y_train, y_pred))) \n",
    "        print(\" test acc : \" + str(accuracy_score(y_test, y_pred))) \n",
    "        print(\" cross val score : \" + str(cross_val_score(loj_model, X_test, y_test, cv = 10).mean())\n",
    "    \n",
    "        \n",
    "        \n",
    "    def class_report(self):\n",
    "              \n",
    "              print(classification_report(y,y_pred))\n",
    "              \n",
    "    def class_report_split(self):\n",
    "              \n",
    "              print(classification_report(y_test,y_pred))\n",
    "              \n",
    "    def confusion_matrix(self):\n",
    "        \n",
    "        mtx = confusion_matrix(y, y_pred)\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        sns.heatmap(mtx, xticklabels= 'auto', yticklabels='auto', cmap=\"Blues\", annot=True, fmt='d', linewidths=.5,  cbar=False, ax=ax)\n",
    "        #  square=True,\n",
    "        plt.ylabel('true label')\n",
    "        plt.xlabel('predicted label')\n",
    "              \n",
    "    def confusion_matrix_split_train(self):\n",
    "              \n",
    "              mtx = confusion_matrix(y_train, y_pred)\n",
    "              fig, ax = plt.subplots(figsize=(5,5))\n",
    "              sns.heatmap(mtx, xticklabels= 'auto', yticklabels='auto', cmap=\"Blues\", annot=True, fmt='d', linewidths=.5,  cbar=False, ax=ax)\n",
    "              #  square=True,\n",
    "              plt.ylabel('true label')\n",
    "              plt.xlabel('predicted label')\n",
    "              \n",
    "              \n",
    "    def confusion_matrix_split_test(self):\n",
    "              \n",
    "              mtx = confusion_matrix(y_test, y_pred)\n",
    "              fig, ax = plt.subplots(figsize=(5,5))\n",
    "              sns.heatmap(mtx, xticklabels= 'auto', yticklabels='auto', cmap=\"Blues\", annot=True, fmt='d', linewidths=.5,  cbar=False, ax=ax)\n",
    "              #  square=True,\n",
    "              plt.ylabel('true label')\n",
    "              plt.xlabel('predicted label')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def auc_roc(self):\n",
    "              \n",
    "              print(\"auc alan değeri:\" + str(roc_auc_score(y,y_pred)))\n",
    "              \n",
    "              fpr, tpr, thresholds = roc_curve(y, loj_model.predict_proba(X)[:,1])\n",
    "              plt.figure()\n",
    "              plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\n",
    "              plt.plot([0, 1], [0, 1],'r--')\n",
    "              plt.xlim([0.0, 1.0])\n",
    "              plt.ylim([0.0, 1.05])\n",
    "              plt.xlabel('False Positive Oranı')\n",
    "              plt.ylabel('True Positive Oranı')\n",
    "              plt.title('ROC')\n",
    "              plt.show()\n",
    "        \n",
    "    \n",
    "    def auc_roc_split(self):\n",
    "              \n",
    "              print(\"auc alan değeri:\" + str(roc_auc_score(y_test,y_pred)))\n",
    "              \n",
    "              fpr, tpr, thresholds = roc_curve(y_test, loj_model.predict_proba(X_test)[:,1])\n",
    "              plt.figure()\n",
    "              plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\n",
    "              plt.plot([0, 1], [0, 1],'r--')\n",
    "              plt.xlim([0.0, 1.0])\n",
    "              plt.ylim([0.0, 1.05])\n",
    "              plt.xlabel('False Positive Oranı')\n",
    "              plt.ylabel('True Positive Oranı')\n",
    "              plt.title('ROC')\n",
    "              plt.show()\n",
    "              \n",
    "              \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
